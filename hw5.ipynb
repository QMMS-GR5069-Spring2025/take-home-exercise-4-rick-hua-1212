{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028f79cb-f7f5-4b99-9e2d-e1a102131b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6dddcf7-4292-4725-985b-2bd4610cee50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 加载 CSV 数据\n",
    "\n",
    "# df_races = spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header=True)\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header=True)\n",
    "# df_driver = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header=True)\n",
    "# df_pit_stops = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb500a8-8ae7-49ba-ab73-1c4e2a2bd7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f2fbba-e70d-433d-8241-74d9feaef325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS model1_predictions (\n",
    "    raceId INT,\n",
    "    driverId INT,\n",
    "    prediction DOUBLE,\n",
    "    probability DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS model2_predictions (\n",
    "    raceId INT,\n",
    "    driverId INT,\n",
    "    prediction DOUBLE,\n",
    "    probability DOUBLE\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c09ab1-32cd-445e-858f-ca9e47e994c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 选择几个有用的特征（你可以根据具体字段做更多清洗）\n",
    "df = df_results.select(\n",
    "    col('raceId').cast(IntegerType()),\n",
    "    col('driverId').cast(IntegerType()),\n",
    "    col('constructorId').cast(IntegerType()),\n",
    "    col('grid').cast(IntegerType()),\n",
    "    col('positionOrder').cast(IntegerType()).alias('label')  # 将名次作为标签\n",
    ").na.drop()\n",
    "\n",
    "# 特征组装\n",
    "assembler = VectorAssembler(inputCols=['constructorId', 'grid'], outputCol='features')\n",
    "df_features = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd5dfaa-33f9-42ea-a9c4-9df3e26aca22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows with null values\n",
    "df_results = df_results.dropna(subset=[\n",
    "    \"grid\", \"points\", \"laps\", \"milliseconds\", \"fastestLap\", \"rank\", \"fastestLapSpeed\", \"positionOrder\"\n",
    "])\n",
    "\n",
    "df_results = df_results.select(\n",
    "    col(\"grid\").cast(\"double\"),\n",
    "    col(\"points\").cast(\"double\"),\n",
    "    col(\"laps\").cast(\"double\"),\n",
    "    col(\"milliseconds\").cast(\"double\"),\n",
    "    col(\"fastestLap\").cast(\"double\"),\n",
    "    col(\"rank\").cast(\"double\"),\n",
    "    col(\"fastestLapSpeed\").cast(\"double\"),\n",
    "    col(\"positionOrder\").cast(\"double\")\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de150ea5-94d5-4502-aba0-b5d09dd7118e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 选择特征和标签列\n",
    "feature_cols = ['grid', 'points', 'laps', 'milliseconds', 'fastestLap', 'rank', 'fastestLapSpeed']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "data = assembler.transform(df_results).select(\"features\", \"positionOrder\")\n",
    "\n",
    "# 拆分训练集和测试集\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6faaa7-36a6-418d-8eb3-3f9afc7d0731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "with mlflow.start_run(run_name=\"rf_model_v1\"):\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"positionOrder\", numTrees=50, maxDepth=10)\n",
    "    model = rf.fit(train_data)\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"positionOrder\", predictionCol=\"prediction\")\n",
    "\n",
    "    # 记录参数\n",
    "    mlflow.log_param(\"numTrees\", 50)\n",
    "    mlflow.log_param(\"maxDepth\", 10)\n",
    "\n",
    "    # 记录四个评估指标\n",
    "    mlflow.log_metric(\"accuracy\", evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "    mlflow.log_metric(\"f1\", evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))\n",
    "    mlflow.log_metric(\"precision\", evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "    mlflow.log_metric(\"recall\", evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"}))\n",
    "\n",
    "    # 记录模型\n",
    "    mlflow.spark.log_model(model, \"random_forest_model\")\n",
    "\n",
    "    # 记录两个 artifacts（示例）\n",
    "    with open(\"/tmp/summary.txt\", \"w\") as f:\n",
    "        f.write(\"Random Forest with engineered race features\")\n",
    "    mlflow.log_artifact(\"/tmp/summary.txt\")\n",
    "\n",
    "    # 保存 feature 列表\n",
    "    with open(\"/tmp/features.txt\", \"w\") as f:\n",
    "        f.write(\",\".join(feature_cols))\n",
    "    mlflow.log_artifact(\"/tmp/features.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47ba2bc-6611-445c-ac10-108cd7066ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\", \"positionOrder\", \"features\") \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"model1_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f86e88-fd12-4c59-897e-f1c456c05905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "with mlflow.start_run(run_name=\"logreg_model_v2\"):\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"positionOrder\", maxIter=50, regParam=0.1)\n",
    "    model = lr.fit(train_data)\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"positionOrder\", predictionCol=\"prediction\")\n",
    "\n",
    "    # 记录参数\n",
    "    mlflow.log_param(\"maxIter\", 50)\n",
    "    mlflow.log_param(\"regParam\", 0.1)\n",
    "\n",
    "    # 记录评估指标\n",
    "    mlflow.log_metric(\"accuracy\", evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n",
    "    mlflow.log_metric(\"f1\", evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))\n",
    "    mlflow.log_metric(\"precision\", evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "    mlflow.log_metric(\"recall\", evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"}))\n",
    "\n",
    "    # 记录模型\n",
    "    mlflow.spark.log_model(model, \"logreg_model\")\n",
    "\n",
    "    # 记录 artifacts\n",
    "    with open(\"/tmp/summary_logreg.txt\", \"w\") as f:\n",
    "        f.write(\"Multiclass Logistic Regression using positionOrder\")\n",
    "    mlflow.log_artifact(\"/tmp/summary_logreg.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13868cb-a09a-4b86-8434-b8cbd23f5f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\", \"positionOrder\", \"features\") \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"model2_predictions\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
